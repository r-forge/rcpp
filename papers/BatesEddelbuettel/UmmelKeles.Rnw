\documentclass[11pt,letterpaper]{article}
\usepackage[top=1.2in,left=0.9in,bottom=0.8in]{geometry}
\usepackage{paralist,mdwlist,amsmath,amsfonts,amsbsy,graphicx,alltt,fancyhdr,Sweave,bm}
\SweaveOpts{engine=R,eps=FALSE,width=10,height=6.5,strip.white=all,keep.source=TRUE}
\SweaveOpts{prefix=TRUE,prefix.string=figs/,include=TRUE}
\setkeys{Gin}{width=0.8\textwidth}
\DefineVerbatimEnvironment{Sinput}{Verbatim}
{formatcom={\vspace{-1ex}},fontshape=sl,
  fontfamily=courier,fontseries=b, fontsize=\small}
\DefineVerbatimEnvironment{Soutput}{Verbatim}
{formatcom={\vspace{-2ex}},fontfamily=courier,fontseries=b,fontsize=\small}
\lhead{\sf Rcpp Example}
\rhead{\sf 2011-04-22 (p. \thepage)}
\lfoot{}\cfoot{}\rfoot{}
\pagestyle{fancy}
\newcommand{\code}[1]{\texttt{\small #1}}
\newcommand{\R}{\textsf{R}}
<<initial,echo=FALSE,print=FALSE>>=
library(inline)
library(Rcpp)
library(rbenchmark)
N1 <- 1e5
N2 <- 1e7
@
\begin{document}

Recently a query by Kevin Ummel on the R-help mailing list prompted a
discussion of a problem that boils down to comparing the elements of
two numeric vectors, \code{x} and \code{y}, and determining for each
element in one vector the number of elements in the second vector that
are less than or equal to it.

There are various ways of doing this.  The original poster used
<<f1>>=
f1 <- function(x, y)
    sapply(x, function(i) length(which(y < i)))
@

Richard Heiberger and Marc Schwartz both suggested
<<f2>>=
f2 <- function(x, y)
    colSums(outer(y, x, '<'))
@

Gustavo Carvalho suggested the equivalent of
<<f3>>=
f3 <- Vectorize(function(x, y) sum(y < x), "x")
@
and Bill Dunlap, drawing on his encyclopedic knowledge of S-PLUS and R
functions, noted that this operation was essential what is done in R's
\code{findInterval} function which uses compiled code implementing a binary search.
<<f4>>=
f4 <- function(x, y) length(y) - findInterval(-x, rev(-sort(y)))
@

For large vectors \code{x} and \code{y}, Bill's version is much faster
than any of the other suggestions which involve comparing each element
of \code{x} to each elements of \code{y}. Interestingly, the second
version (\code{f2}), which was suggested by two experienced R users,
can become deadly slow on moderate sized vectors, because of the way
that the \code{outer} function is implemented.

Even with moderate sized vectors [...?...]

<<compPre,print=FALSE,echo=FALSE,results=hide>>=
set.seed(1)
x <- rnorm(5000)
y <- rnorm(20000)
#x <- rnorm(500)
#y <- rnorm(2000)
system.time(a1 <- f1(x, y))
system.time(a2 <- f2(x, y))
system.time(a3 <- f3(x, y))
system.time(a4 <- f4(x, y))
all.equal(a1, a2)
all.equal(a1, a3)
all.equal(a1, a4)
benchmark(f1(x,y), f2(x,y), f3(x,y), f4(x,y),
          columns=c("test","elapsed","relative"),
          order="relative", replications=10L)

@

<<comp>>=
benchmark(f1(x,y), f2(x,y), f3(x,y), f4(x,y),
          columns=c("test","elapsed","relative"),
          order="relative", replications=10L)
@

We will eliminate all but Bill Dunlap's method on this evidence and
change the rules a bit.  The question posed by Sunduz Keles regarded
$p$-values from a test sample relative to a larger reference sample.
%
%(From here you can continue with the description on the R-help and
%Rcpp-Devel postings in which I included benchmark timings of various
%versions.)
%
% My colleague Sunduz Keles once mentioned a similar problem to me.
She had a large sample from a reference distribution and a test sample
(both real-valued in her case) and she wanted, for each element of the
test sample, the proportion of the reference sample that was less than
the element.  It's a type of empirical $p$-value calculation.

A first C++ approach uses \code{std::upper\_bound} for unordered $x$:
<<upperBoundUnOrdered>>=
## version using std::upper_bound for unordered x
h <- cxxfunction(signature(x_="numeric", y_="numeric"), '
{
    Rcpp::NumericVector x(x_),
        y = clone(Rcpp::NumericVector(y_));
    int n = x.size();
    Rcpp::IntegerVector ans(n);
    const Rcpp::NumericVector::iterator bb = y.begin(), ee = y.end();
               // sort (a copy of) the y values for binary search
    std::sort(bb, ee);

    for (int i = 0; i < n; i++) {
	ans[i] = std::upper_bound(bb, ee, x[i]) - bb;
    }
    return ans;
}
', plugin="Rcpp")
@


The next C++ solution assumes $x$ is non-decreasing, and still uses
\code{std::upper\_bound}:

<<upperBoundOrdered>>=
## version using std::upper_bound when x is non-decreasing
j1 <- cxxfunction(signature(x_="numeric", y_="numeric"), '
{
    Rcpp::NumericVector x(x_),
        y = clone(Rcpp::NumericVector(y_));
    int n = x.size();
    Rcpp::IntegerVector ans(n);
    const Rcpp::NumericVector::iterator bb = y.begin(), ee = y.end();
    Rcpp::NumericVector::iterator mm = y.begin();
               // sort (a copy of) the y values for binary search
    std::sort(bb, ee);

    for (int i = 0; i < n; i++) {
        mm = std::upper_bound(mm, ee, x[i]);
        ans[i] = mm - bb;
    }
    return ans;
}
', plugin="Rcpp")

j <- function(x, y) {
    ord <- order(x)
    ans <- integer(length(x))
    ans[ord] <- j1(x[ord], y)
    ans
}
@

The third C++ solution uses a sequential search:

<<seqSearch>>=
## version using sequential search
k1 <- cxxfunction(signature(xs_="numeric", y_="numeric", ord_="integer"), '
{
    Rcpp::NumericVector xs(xs_),
        y = clone(Rcpp::NumericVector(y_));
    int n = xs.size();
    Rcpp::IntegerVector ord(ord_), ans(n);
    const Rcpp::NumericVector::iterator bb = y.begin(), ee = y.end();
    Rcpp::NumericVector::iterator yy = y.begin();
    double *xp = xs.begin();
               // sort (a copy of) the y values for binary search
    std::sort(bb, ee);

    for (int i = 0; i < n && yy < ee; i++, xp++) {
	while (*xp >= *yy && yy < ee) yy++;
	ans[ord[i] - 1] = yy - bb;
    }
    return ans;
}
', plugin="Rcpp")

k <- function(x, y) {
    ord <- order(x)
    k1(x[ord], y, ord)
}
@

We evaluate all methods on random data of dimension $\Sexpr{N1}$ and
$\Sexpr{N2}$. We also ensure results are identical.

<<comparison,print=FALSE,echo=FALSE,results=hide>>=
x <- rnorm(N1)
yy <- y <- rnorm(N2)
yy[1] <- y[1]                           # ensure y and yy are distinct

aa <- f4(x, y)
all.equal(aa, h(x, y))
all.equal(aa, j(x, y))
all.equal(aa, k(x, y))
all.equal(y, yy)                        # check that nothing has changed in y

res <- benchmark(f4(x,y), h(x,y), j(x,y), k(x,y),
                 order="relative", replications=10,
                 columns=c("test", "replications", "elapsed", "relative"))
@

Finally, the timing comparison:
<<comparisonRes>>=
res
@

The three C++ solutions are reasonably close in performances.

\end{document}
